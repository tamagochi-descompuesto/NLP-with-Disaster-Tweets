{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning, integration and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import f1_score\n",
    "from spellchecker import SpellChecker\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data.\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spellings(text: str) -> str:\n",
    "    \"\"\"\n",
    "        Function that corrects the spelling of a given text.\n",
    "        \n",
    "        Parameters:\n",
    "        text - A string containing the text to correct.\n",
    "\n",
    "        Returns:\n",
    "        text - The corrected input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a SpellChekcer object and use it to identify misspelled words\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "\n",
    "    # Correct and appénd misspelled words\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_antonym(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "        Function that corrects the spelling of a given text.\n",
    "        \n",
    "        Parameters:\n",
    "        text - A string containing the text to correct.\n",
    "\n",
    "        Returns:\n",
    "        text - The corrected input text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input and define a list with the corrected words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    temp_word = ''\n",
    "\n",
    "    for word in words:\n",
    "        antonyms = []\n",
    "        \n",
    "        if word == 'not':\n",
    "            # Set temporary word flag if current word is not\n",
    "            temp_word = 'not_'\n",
    "        elif temp_word == 'not_':\n",
    "            # Find antonyms for the current word using WordNet\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for s in syn.lemmas():\n",
    "                    for a in s.antonyms():\n",
    "                        antonyms.append(a.name())\n",
    "            \n",
    "            # Replace current word with its antonym if found, else append not\n",
    "            if len(antonyms) >= 1:\n",
    "                word = antonyms[0]\n",
    "            else:\n",
    "                word = temp_word + word # when antonym is not found, it will\n",
    "                                    # remain not_happy\n",
    "            temp_word = ''\n",
    "        \n",
    "        if word != 'not':\n",
    "            # Add current word tom the list of corrected words if it is not 'not'\n",
    "            new_words.append(word)\n",
    "\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "        Function that cleans a given text.\n",
    "        \n",
    "        Parameters:\n",
    "        text - A string containing the text to clean.\n",
    "\n",
    "        Returns:\n",
    "        text - The cleaned input text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower() # Lowercase text\n",
    "    text= re.sub(r'[^\\w\\s]',' ',text) # Removing everything other than space and words\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text ) # Removing URLs\n",
    "    text= re.sub(r'[0-9]',' ',text) # Removing digits\n",
    "    #text = correct_spellings(text) # Correcting the spelling of misspelled tweets\n",
    "    text = convert_to_antonym(text) # Change negative words to its antonym i.e. not happy -> unhapy\n",
    "    text = re.sub(' +', ' ', text) # Correcting double space text   \n",
    "     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean train and test text\n",
    "train_text = [clean_text(text) for text in train_df['text']]\n",
    "test_text = [clean_text(text) for text in test_df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our deeds are the reason of this earthquake may allah forgive us all',\n",
       " 'forest fire near la ronge sask canada',\n",
       " 'all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected',\n",
       " 'people receive wildfires evacuation orders in california',\n",
       " 'just got sent this photo from ruby alaska as smoke from wildfires pours into a school',\n",
       " 'rockyfire update california hwy closed in both directions due to lake county fire cafire wildfires',\n",
       " 'flood disaster heavy rain causes flash flooding of streets in manitou colorado springs areas',\n",
       " 'i m on top of the hill and i can see a fire in the woods',\n",
       " 'there s an emergency evacuation happening now in the building across the street',\n",
       " 'i m afraid that the tornado is coming to our area',\n",
       " 'three people died from the heat wave so far']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just happened a terrible car crash',\n",
       " 'heard about earthquake is different cities stay safe everyone',\n",
       " 'there is a forest fire at spot pond geese are fleeing across the street i can not_save them all',\n",
       " 'apocalypse lighting spokane wildfires',\n",
       " 'typhoon soudelor kills in china and taiwan',\n",
       " 'we re shaking it s an earthquake',\n",
       " 'they d probably still show more life than arsenal did yesterday eh eh',\n",
       " 'hey how are you',\n",
       " 'what a nice hat',\n",
       " 'fuck off',\n",
       " 'no i don t like cold']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  our deeds are the reason of this earthquake ma...\n",
       "1              forest fire near la ronge sask canada\n",
       "2  all residents asked to shelter in place are be...\n",
       "3  people receive wildfires evacuation orders in ...\n",
       "4  just got sent this photo from ruby alaska as s..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate corpus\n",
    "corpus = pd.DataFrame(columns=['text'])\n",
    "corpus['text'] = pd.concat([pd.Series(train_text), pd.Series(test_text)])\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(ngram_range=(1, 3), stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(ngram_range=(1, 3), stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 3), stop_words='english')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate TFIDF vectorizer with just unigrams and train it over the corpus\n",
    "tfidf_vectorizer_unigram = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 1))\n",
    "tfidf_vectorizer_unigram.fit(corpus['text'])\n",
    "\n",
    "# Generate TFIDF vectorizer with just unigrams and bigrams and train it over the corpus\n",
    "tfidf_vectorizer_bigram = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 2))\n",
    "tfidf_vectorizer_bigram.fit(corpus['text'])\n",
    "\n",
    "# Generate TFIDF vectorizer with just unigrams, bigrams and trigrams and train it over the corpus\n",
    "tfidf_vectorizer_trigram = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 3))\n",
    "tfidf_vectorizer_trigram.fit(corpus['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train text to vector using TFIDF vectorizers\n",
    "train_tfidfvectors_unigram = tfidf_vectorizer_unigram.transform(train_df['text'])\n",
    "train_tfidfvectors_bigram = tfidf_vectorizer_bigram.transform(train_df['text'])\n",
    "train_tfidfvectors_trigram = tfidf_vectorizer_trigram.transform(train_df['text'])\n",
    "\n",
    "# Same for test text\n",
    "test_tfidfvectors_unigram = tfidf_vectorizer_unigram.transform(test_df['text'])\n",
    "test_tfidfvectors_bigram = tfidf_vectorizer_bigram.transform(test_df['text'])\n",
    "test_tfidfvectors_trigram = tfidf_vectorizer_trigram.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Shapes ------\n",
      "(7613, 28562)\n",
      "(7613, 102837)\n",
      "(7613, 175856)\n",
      "\n",
      "(3263, 28562)\n",
      "(3263, 102837)\n",
      "(3263, 175856)\n"
     ]
    }
   ],
   "source": [
    "print('------ Shapes ------')\n",
    "print(train_tfidfvectors_unigram.shape)\n",
    "print(train_tfidfvectors_bigram.shape)\n",
    "print(train_tfidfvectors_trigram.shape)\n",
    "print()\n",
    "print(test_tfidfvectors_unigram.shape)\n",
    "print(test_tfidfvectors_bigram.shape)\n",
    "print(test_tfidfvectors_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Singular Value Decomposition to reduce dimensionality\n",
    "tsv = TruncatedSVD(n_components=500)\n",
    "\n",
    "train_tfidfvectors_unigram_svd = tsv.fit_transform(train_tfidfvectors_unigram)\n",
    "train_tfidfvectors_bigram_svd = tsv.fit_transform(train_tfidfvectors_bigram)\n",
    "train_tfidfvectors_trigram_svd = tsv.fit_transform(train_tfidfvectors_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Shapes ------\n",
      "(7613, 500)\n",
      "(7613, 500)\n",
      "(7613, 500)\n"
     ]
    }
   ],
   "source": [
    "print('------ Shapes ------')\n",
    "print(train_tfidfvectors_unigram_svd.shape)\n",
    "print(train_tfidfvectors_bigram_svd.shape)\n",
    "print(train_tfidfvectors_trigram_svd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "data = [train_tfidfvectors_unigram, \n",
    "        train_tfidfvectors_bigram, \n",
    "        train_tfidfvectors_trigram,\n",
    "        train_tfidfvectors_unigram_svd,\n",
    "        train_tfidfvectors_bigram_svd,\n",
    "        train_tfidfvectors_trigram_svd]\n",
    "\n",
    "# Iterate over each vectorized dataset\n",
    "for index, X in enumerate(data):\n",
    "    # Initialize an empty list to store F1 scores for each fold\n",
    "    f1_scores = []\n",
    "\n",
    "    # Iterate over each fold \n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index].toarray(), X[val_index].toarray()\n",
    "        y_train, y_val = train_df['target'][train_index], train_df['target'][val_index]\n",
    "\n",
    "\n",
    "        # Reshape input data to treat each row as a single timestep\n",
    "        X_train = X_train[:, :, np.newaxis]\n",
    "        X_val = X_val[:, :, np.newaxis]\n",
    "        \n",
    "        # Build the RNN model\n",
    "        model = Sequential([\n",
    "            SimpleRNN(32, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),  # RNN layer with 32 units\n",
    "            Dense(1, activation='sigmoid')  # Dense layer with sigmoid activation for binary classification\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=25, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
    "        \n",
    "        # Predict on the validation data\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)  # Convert predicted probabilities to binary predictions\n",
    "        \n",
    "        # Calculate F1 score and append to the list\n",
    "        f1 = f1_score(y_val.flatten(), y_pred_binary.flatten())\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    ('Scores for dataset', index)\n",
    "    # Print F1 scores for each fold:\n",
    "    print('Individual F1 Scores:', f1_scores)\n",
    "    \n",
    "    # Calculate and print the average F1 score across all folds\n",
    "    avg_f1_score = np.mean(f1_scores)\n",
    "    print(\"Average F1 Score:\", avg_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
